<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      A Traffic Sign Recognition Classifier | Yang Yang 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Yang">
    
    

    <meta name="description" content="#Traffic Sign Recognition Build a Traffic Sign Recognition Project The goals / steps of this project are the following:  Load the data set (see below for links to the project data set) Explore, summa">
<meta property="og:type" content="article">
<meta property="og:title" content="A Traffic Sign Recognition Classifier | Yang Yang">
<meta property="og:url" content="http://yoursite.com/2017/08/15/A Traffic Sign Recognition Classifier/index.html">
<meta property="og:site_name" content="Yang Yang">
<meta property="og:description" content="#Traffic Sign Recognition Build a Traffic Sign Recognition Project The goals / steps of this project are the following:  Load the data set (see below for links to the project data set) Explore, summa">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-0cdf5c73e457c5b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-cdc03f67cec4ab0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-dde4b0abe530cf99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-d03b1da7774e95f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-5c548c3c7d6d5d11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-c530cadf6036ac74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-cbb216b6759e09a3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-36c171a44c48779a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-d1e8332f4ac51049.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-742eca4dcfb0e3f0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-52bb04b910401388.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/150">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-481c393668229d84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-5afcb0e127ffe8c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-c55f8a138044c2a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-3b4aac994522e932.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-33fa1993c88a92c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/2528310-0eb8ce547596f88f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2017-10-05T09:38:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Traffic Sign Recognition Classifier | Yang Yang">
<meta name="twitter:description" content="#Traffic Sign Recognition Build a Traffic Sign Recognition Project The goals / steps of this project are the following:  Load the data set (see below for links to the project data set) Explore, summa">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/2528310-0cdf5c73e457c5b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Yang Yang</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/home" title="" class="">Home</a></li>
              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">Blog</a></li>
              
                
                <li class="navigation__item"><a href="/publication" title="" class="">Publication</a></li>
              
                
                <li class="navigation__item"><a href="/resume" title="" class="">Resume</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">Archive</a></li>
              
                
                <li class="navigation__item"><a href="/contact" title="" class="">Contact</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">A Traffic Sign Recognition Classifier</h1>

    

    <div class="post-meta">
      <time datetime="2017-08-15" class="post-meta__date date">2017-08-15</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/自动驾驶/">自动驾驶</a>
            </font>
          

          

      </span>
            <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <hr>
<h2 id="Traffic-Sign-Recognition"><a href="#Traffic-Sign-Recognition" class="headerlink" title="#Traffic Sign Recognition "></a>#<strong>Traffic Sign Recognition</strong> </h2><p><strong>Build a Traffic Sign Recognition Project</strong></p>
<p>The goals / steps of this project are the following:</p>
<ul>
<li>Load the data set (see below for links to the project data set)</li>
<li>Explore, summarize and visualize the data set</li>
<li>Design, train and test a model architecture</li>
<li>Use the model to make predictions on new images</li>
<li>Analyze the softmax probabilities of the new images</li>
<li>Summarize the results with a written report</li>
</ul>
<hr>
<p>Here is a link to my <a href="https://github.com/fighting41love/Udacity_Traffic_Sign_Classifier/blob/master/Traffic_Sign_Classifier.ipynb" target="_blank" rel="external">project code</a></p>
<p>###Data Set Summary &amp; Exploration</p>
<h4 id="1-Provide-a-basic-summary-of-the-data-set-In-the-code-the-analysis-should-be-done-using-python-numpy-and-or-pandas-methods-rather-than-hardcoding-results-manually"><a href="#1-Provide-a-basic-summary-of-the-data-set-In-the-code-the-analysis-should-be-done-using-python-numpy-and-or-pandas-methods-rather-than-hardcoding-results-manually" class="headerlink" title="1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually."></a>1. Provide a basic summary of the data set. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.</h4><p><strong>Re:</strong> I used the python, numpy and matplotlib library to calculate summary statistics of the traffic signs data set:</p>
<ul>
<li>The size of training set is ?<br>34799 (Is it a large enough dataset? I’m not sure. The traffic signs in different weather, brightness condition are totally different. Image augmentation is good choice to solve the brightness problem. I wonder the whether we should collect a larger dataset.)</li>
<li>The size of the validation set is ?<br>4410</li>
<li>The size of test set is ?<br>12630</li>
<li>The shape of a traffic sign image is ?<br>Image data shape = (32, 32,3)</li>
<li>The number of unique classes/labels in the data set is ?<br>{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42}</li>
</ul>
<h4 id="2-Include-an-exploratory-visualization-of-the-dataset"><a href="#2-Include-an-exploratory-visualization-of-the-dataset" class="headerlink" title="2. Include an exploratory visualization of the dataset."></a>2. Include an exploratory visualization of the dataset.</h4><p><strong>Re:</strong> First, we draw all the classes in the following figure. There are 43 classes in all. As shown in the figure, some images are too dark to figure out the detail of the sign. It’s necessary to preprocess the image into grayscale.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-0cdf5c73e457c5b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="All categories"></p>
<p>Here is an exploratory visualization of the data set. It is a bar chart showing distributions of all the classes in train, validation and test data. We can see that the distributions in the three parts are similar. </p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-cdc03f67cec4ab0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Distribution of classes in train data."></p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-dde4b0abe530cf99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Distribution of classes in validation data."></p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-d03b1da7774e95f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Distribution of classes in train data."></p>
<p>###Design and Test a Model Architecture</p>
<h4 id="3-Describe-how-you-preprocessed-the-image-data-What-techniques-were-chosen-and-why-did-you-choose-these-techniques-Consider-including-images-showing-the-output-of-each-preprocessing-technique-Pre-processing-refers-to-techniques-such-as-converting-to-grayscale-normalization-etc"><a href="#3-Describe-how-you-preprocessed-the-image-data-What-techniques-were-chosen-and-why-did-you-choose-these-techniques-Consider-including-images-showing-the-output-of-each-preprocessing-technique-Pre-processing-refers-to-techniques-such-as-converting-to-grayscale-normalization-etc" class="headerlink" title="3. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc."></a>3. Describe how you preprocessed the image data. What techniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc.</h4><p><strong>Re:</strong> As a first step, I decided to convert the images to grayscale. On the one hand, the color information is not very useful. On the other hand, the gray image has only one channel, which greatly reduces the computation.</p>
<p>Here is an example of a traffic sign image before and after grayscaling.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-5c548c3c7d6d5d11.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Grayscale: before and after"></p>
<p>The codes are as follows:<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Convert to grayscale</span></div><div class="line"><span class="attr">X_train_rgb</span> = X_train</div><div class="line"><span class="attr">X_train_gry</span> = np.sum(X_train/<span class="number">3</span>, axis=<span class="number">3</span>, keepdims=<span class="literal">True</span>)</div><div class="line"></div><div class="line"><span class="attr">X_test_rgb</span> = X_test</div><div class="line"><span class="attr">X_test_gry</span> = np.sum(X_test/<span class="number">3</span>, axis=<span class="number">3</span>, keepdims=<span class="literal">True</span>)</div><div class="line"></div><div class="line"><span class="attr">X_valid_rgb</span> = X_valid</div><div class="line"><span class="attr">X_valid_gry</span> = np.sum(X_valid/<span class="number">3</span>, axis=<span class="number">3</span>, keepdims=<span class="literal">True</span>)</div></pre></td></tr></table></figure></p>
<p>As a last step, I normalized the image data because  the normalization of a relational data can reduce data redundancy and improve data integrity.<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Normalize the train and test datasets to (-1,1)</span></div><div class="line"></div><div class="line"><span class="attr">X_train_normalized</span> = (X_train - <span class="number">128</span>)/<span class="number">128</span> </div><div class="line"><span class="attr">X_test_normalized</span> = (X_test - <span class="number">128</span>)/<span class="number">128</span></div><div class="line"><span class="attr">X_valid_normalized</span> = (X_valid - <span class="number">128</span>)/<span class="number">128</span></div></pre></td></tr></table></figure></p>
<p>The difference between the original data set and the normalized data set is as follows. We cannot tell the difference by looking at the images.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-c530cadf6036ac74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Normalized image."></p>
<h4 id="4-Describe-what-your-final-model-architecture-looks-like-including-model-type-layers-layer-sizes-connectivity-etc-Consider-including-a-diagram-and-or-table-describing-the-final-model"><a href="#4-Describe-what-your-final-model-architecture-looks-like-including-model-type-layers-layer-sizes-connectivity-etc-Consider-including-a-diagram-and-or-table-describing-the-final-model" class="headerlink" title="4. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model."></a>4. Describe what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.</h4><p><strong>Re:</strong> My final model is based on <strong>LeNet architecture</strong>. To improve the performance, we add one more dense layer. The overall model consists of the following layers:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Layer</th>
<th style="text-align:center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Input</td>
<td style="text-align:center">32x32x1 Gray image</td>
</tr>
<tr>
<td style="text-align:center">Convolution 5x5</td>
<td style="text-align:center">1x1 stride, valid padding, outputs 28x28x6</td>
</tr>
<tr>
<td style="text-align:center">RELU</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Max pooling</td>
<td style="text-align:center">2x2 stride,  outputs 14x14x6</td>
</tr>
<tr>
<td style="text-align:center">Convolution 5x5</td>
<td style="text-align:center">1x1 stride, valid padding, outputs 10x10x16</td>
</tr>
<tr>
<td style="text-align:center">RELU</td>
<td style="text-align:center"></td>
<td></td>
</tr>
<tr>
<td style="text-align:center">Max pooling</td>
<td style="text-align:center">2x2 stride,  outputs 5x5x16</td>
</tr>
<tr>
<td style="text-align:center">Flatten</td>
<td style="text-align:center">outputs   400</td>
</tr>
<tr>
<td style="text-align:center">Fully connected</td>
<td style="text-align:center">outputs   256</td>
</tr>
<tr>
<td style="text-align:center">Fully connected</td>
<td style="text-align:center">outputs   120</td>
</tr>
<tr>
<td style="text-align:center">Fully connected</td>
<td style="text-align:center">outputs   84</td>
</tr>
<tr>
<td style="text-align:center">Fully connected</td>
<td style="text-align:center">outputs   43</td>
</tr>
<tr>
<td style="text-align:center">Softmax</td>
<td style="text-align:center">outputs 43                                          </td>
</tr>
</tbody>
</table>
<h4 id="5-Describe-how-you-trained-your-model-The-discussion-can-include-the-type-of-optimizer-the-batch-size-number-of-epochs-and-any-hyperparameters-such-as-learning-rate"><a href="#5-Describe-how-you-trained-your-model-The-discussion-can-include-the-type-of-optimizer-the-batch-size-number-of-epochs-and-any-hyperparameters-such-as-learning-rate" class="headerlink" title="5. Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate."></a>5. Describe how you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.</h4><p><strong>Re:</strong> To train the model, I used an Adam optimizer with learning rate 0.001. Learning rate 0.01 is too large, the CNN cannot learn anything from the data. Learning rate 0.0001 is too small, which consumes much time in training. The batch size is set to 128. Larger batch size costs much time. The number of epochs is 100. Actually, the model converges about after 35 epochs.</p>
<h4 id="6-Describe-the-approach-taken-for-finding-a-solution-and-getting-the-validation-set-accuracy-to-be-at-least-0-93-Include-in-the-discussion-the-results-on-the-training-validation-and-test-sets-and-where-in-the-code-these-were-calculated"><a href="#6-Describe-the-approach-taken-for-finding-a-solution-and-getting-the-validation-set-accuracy-to-be-at-least-0-93-Include-in-the-discussion-the-results-on-the-training-validation-and-test-sets-and-where-in-the-code-these-were-calculated" class="headerlink" title="6. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated."></a>6. Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated.</h4><p>My final model results were:</p>
<ul>
<li>training set accuracy of ?<br><strong>Re:</strong> 96.2%</li>
<li>validation set accuracy of ?<br><strong>Re:</strong> 95.9%</li>
<li>test set accuracy of ?<br><strong>Re:</strong> 93.8%</li>
</ul>
<p>If a well known architecture was chosen:</p>
<ul>
<li>What architecture was chosen?<br><strong>Re:</strong> We choose the <strong>LeNet</strong> to classify the images.</li>
<li><p>Why did you believe it would be relevant to the traffic sign application?<br><strong>Re:</strong> LeNet is simple and it works well on the Minist dataset, which is also a multi-classification task.</p>
</li>
<li><p>How does the final model’s accuracy on the training, validation and test set provide evidence that the model is working well?<br><strong>Re:</strong> The final model’s accuracy  the training, validation and test set are similar. Hence, it works well. If the training acc is much larger than the validation and test acc, it is over-fitting. If all the acc are small, it is under-fitting. For the over-fitting case, we can add dropout and regularizer. For the under-fitting case, we can add more fully connected layers to get better performance.</p>
</li>
</ul>
<p>###Test a Model on New Images</p>
<h4 id="7-Choose-nine-German-traffic-signs-found-on-the-web-and-provide-them-in-the-report-For-each-image-discuss-what-quality-or-qualities-might-be-difficult-to-classify"><a href="#7-Choose-nine-German-traffic-signs-found-on-the-web-and-provide-them-in-the-report-For-each-image-discuss-what-quality-or-qualities-might-be-difficult-to-classify" class="headerlink" title="7. Choose nine German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify."></a>7. Choose nine German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.</h4><p><strong>Re:</strong> Here are five German traffic signs that I found on the web. All the images are high resolution. To feed these five images to our LeNet model, we first resize the image into (32,32,3). The pre-process of the images is similar to the training data.<br><img src="http://upload-images.jianshu.io/upload_images/2528310-cbb216b6759e09a3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150" alt="Yield"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-36c171a44c48779a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150" alt="General danger"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-d1e8332f4ac51049.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150" alt="Priority road    "><br><img src="http://upload-images.jianshu.io/upload_images/2528310-742eca4dcfb0e3f0.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/150" alt="Turn right"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-52bb04b910401388.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/150" alt="Stop Sign"></p>
<p>In my opinion, all the traffic signs in the images are very easy to identify. However, the experimental results are not the situation that we imagined.</p>
<h4 id="8-Discuss-the-model’s-predictions-on-these-new-traffic-signs-and-compare-the-results-to-predicting-on-the-test-set-At-a-minimum-discuss-what-the-predictions-were-the-accuracy-on-these-new-predictions-and-compare-the-accuracy-to-the-accuracy-on-the-test-set-OPTIONAL-Discuss-the-results-in-more-detail-as-described-in-the-“Stand-Out-Suggestions”-part-of-the-rubric"><a href="#8-Discuss-the-model’s-predictions-on-these-new-traffic-signs-and-compare-the-results-to-predicting-on-the-test-set-At-a-minimum-discuss-what-the-predictions-were-the-accuracy-on-these-new-predictions-and-compare-the-accuracy-to-the-accuracy-on-the-test-set-OPTIONAL-Discuss-the-results-in-more-detail-as-described-in-the-“Stand-Out-Suggestions”-part-of-the-rubric" class="headerlink" title="8. Discuss the model’s predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the “Stand Out Suggestions” part of the rubric)."></a>8. Discuss the model’s predictions on these new traffic signs and compare the results to predicting on the test set. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the “Stand Out Suggestions” part of the rubric).</h4><p>Here are the results of the prediction:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Image</th>
<th style="text-align:center">Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Yield</td>
<td style="text-align:center">Traffic lights ahead</td>
</tr>
<tr>
<td style="text-align:center">General danger</td>
<td style="text-align:center">Traffic lights ahead</td>
</tr>
<tr>
<td style="text-align:center">Priority road</td>
<td style="text-align:center">Traffic lights ahead</td>
</tr>
<tr>
<td style="text-align:center">Turn right</td>
<td style="text-align:center">Traffic lights ahead</td>
</tr>
<tr>
<td style="text-align:center">Stop Sign</td>
<td style="text-align:center">Traffic lights ahead</td>
</tr>
</tbody>
</table>
<p><strong>Re:</strong> The model was able to correctly guess 0 of the 5 traffic signs, which gives an accuracy of 0%. The problem is that the data is too small, i.e., the resolution of the image is only 32×32. The model may perform well on larger images. We didn’t use augmentation technique. In addition, the LeNet is not robust. In other words, the convolutional neural network is not robust against noises and simple tansformations. Yan Lecun published a paper that demonstrate that the neural network cannot identify some traffic signs after they take a photo and feed the photo into CNN. Hence, they proposed the adversarial training method to make the model robust.  It’s a very hot topic recently. In the future, we can also try the adversarial training method to improve the performance.</p>
<h4 id="9-Describe-how-certain-the-model-is-when-predicting-on-each-of-the-five-new-images-by-looking-at-the-softmax-probabilities-for-each-prediction-Provide-the-top-5-softmax-probabilities-for-each-image-along-with-the-sign-type-of-each-probability"><a href="#9-Describe-how-certain-the-model-is-when-predicting-on-each-of-the-five-new-images-by-looking-at-the-softmax-probabilities-for-each-prediction-Provide-the-top-5-softmax-probabilities-for-each-image-along-with-the-sign-type-of-each-probability" class="headerlink" title="9. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability."></a>9. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction. Provide the top 5 softmax probabilities for each image along with the sign type of each probability.</h4><p><strong>Re:</strong> The code for making predictions on my final model is located in the 60th cell of the Ipython notebook.</p>
<p>For the first image, the model is relatively sure that this is a stop sign (probability of 0.4). The top five soft max probabilities were</p>
<table>
<thead>
<tr>
<th style="text-align:center">Probability</th>
<th style="text-align:center">Prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">.60</td>
<td style="text-align:center">General danger</td>
</tr>
<tr>
<td style="text-align:center">.20</td>
<td style="text-align:center">Speed limit 70</td>
</tr>
<tr>
<td style="text-align:center">.05</td>
<td style="text-align:center">Bend</td>
</tr>
<tr>
<td style="text-align:center">.04</td>
<td style="text-align:center">Bumpy Road</td>
</tr>
<tr>
<td style="text-align:center">.01</td>
<td style="text-align:center">Slippery Road</td>
</tr>
</tbody>
</table>
<h3 id="Optional-Visualizing-the-Neural-Network-See-Step-4-of-the-Ipython-notebook-for-more-details"><a href="#Optional-Visualizing-the-Neural-Network-See-Step-4-of-the-Ipython-notebook-for-more-details" class="headerlink" title="(Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)"></a>(Optional) Visualizing the Neural Network (See Step 4 of the Ipython notebook for more details)</h3><h4 id="10-Discuss-the-visual-output-of-your-trained-network’s-feature-maps-What-characteristics-did-the-neural-network-use-to-make-classifications"><a href="#10-Discuss-the-visual-output-of-your-trained-network’s-feature-maps-What-characteristics-did-the-neural-network-use-to-make-classifications" class="headerlink" title="10. Discuss the visual output of your trained network’s feature maps. What characteristics did the neural network use to make classifications?"></a>10. Discuss the visual output of your trained network’s feature maps. What characteristics did the neural network use to make classifications?</h4><p><strong>Re:</strong> The feature maps learn many high level features from the images. For instance, the shape of the sign, and the edges of numbers on the sign.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/2528310-481c393668229d84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Convolution layer1"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-5afcb0e127ffe8c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Convolution relu1"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-c55f8a138044c2a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Convolution maxpool1"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-3b4aac994522e932.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Convolution layer2"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-33fa1993c88a92c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Convolution relu2"><br><img src="http://upload-images.jianshu.io/upload_images/2528310-0eb8ce547596f88f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Convolution maxpool2"></p>

  </section>

  

<section class="post-comments">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      var disqus_shortname = 'yangyangfuture'; 
      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>


  

<section class="post-comments">

    <div class="ds-thread" data-thread-key="2017/08/15/A Traffic Sign Recognition Classifier/"></div>

    <script type="text/javascript">
      var duoshuoQuery = {short_name:"fighting41love"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
        || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
    </script> 

</section>


</article>


            <footer class="footer">
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

 
</footer>

        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    
    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>
